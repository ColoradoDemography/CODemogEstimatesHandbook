# Database Manual

# Outline

1. Database Manual
2. Organization
3. Table Definitions
4. Reports and Selectors
5. Macros
6. Archiving


# Organization and Table Definitions

The database is organized using the "Custom" method provided by Access, which allows for user defined categories.  The categories are chosen to make it more efficient to find input data, queries, or macros than default organizations.  

### Estimate_Masters

These are the master tables for all of the estimates.  Any data request or final table eventually links back to these tables.  Internal to the database, these are the gold-standard for each geography.

20XXCTFEstimatefinal2 - This bizarrely named table holds the special district final estimates and gets created each year with a new year.  This could probably be changed in future vintages because it's not necessary to rename with the year, but so far I haven't.



BGU_Estimates_2010 - While the District estimates are in the CTFEstimatesfinal2 table, this is where they get generated.  Each BGU (Basic Geographic Unit) is in this table and the final estimates from this table are used to generate and populate the estimates tables in Oracle used to distribute CTF dollars.   BGUs break the state up into unique geographies based on county, municipality, and special district boundaries. The table contains estimates from 2010 to the most recent vintage.



CityandCounty_AnnualConstructionData_2010 - This table is created using construction data input from the RCS and the Census Bureau.  It is also the table where we set which housing data source to use for each municipality.  This is the best table to use as a start for investigating issues with housing inputs, but doesn't hold any final estimate information.  The table contains data from 2010 to the most recent vintage.



CityandCounty_Estimate2010_Master - This is likely the most useful of all the Masters because it contains the final estimates for all municipalities and counties, although it only shows housing unit method components (not births, deaths, and migration).  This table contains out final estimates for all housing data and population data on municipalities.



County_Estimates_Master_2010 - This is the gold standard table.  It contains components and final estimates for all counties and the state.  Since the counties are the gold standard for our estimates (everything must sum up to counties, and we generate the state by summing the counties), this table is incredibly important.  

### County Estimates Queries

This section contains all the queries to generate the County_Estimates_Master_2010 table.  A run-down of these queries is available in the section on County Population estimates.

### Annual Construction Data Queries

This section contains all queries used to build the CityandCounty_AnnualConstructionData_2010 master table.  These queries take construction data inputs and place them into the proper columns in the table, but they DO NOT change the source columns, that is done manually.

### City and County Estimates Queries

This section contains all queries to generate the CityandCounty_Estimates2010_Master table.  More information on these queries is available in the section on City and County estimates.

### BGU Estimates Queries

This section contains queries to create the BGU_Estimates_2010 table.  It gets larger every year since we have to add another full set of queries per year.  More information is available in the section on BGU estimates.

###  BGU New District Data

This section contains tables and queries to assist in appending data to the proper tables to add districts to the CTF process.  You'll only need this once.  See the BGU estimates sections for more information.

CTFDistrict_NewBGUAppends - This table contains the 2010 BGU data for all new districts.  This data is generated by the GIS person when we are notified of a new district.

CTFDistricts_ExistingBGUUpdates - This data 

### CTF Estimates Final Queries

This section contains queries to create the 20XXCTFEstimatesfinal2 table.  More information is available in the section on CTF estimates.

### CTF IT Table Queries

This section contains the queries to generate the tables we send to Oracle for the CTF distribution.  They are updated each year to link to the current vintage's final year then uploaded by Mark Krudwig.  More information is available in the Section on CTF estimates.

### CTF Tables for IT

These tables are what we use for the CTF distribution upload.

DLG_CTF_BGU_COUNTY_TEMP  - This is an Oracle 'holding' table containing the most recent (v2015 at writing) county estimates.  This is used as a control for evaluating the BGU table.

DLG_CTF_BGU_MUNI_TEMP - This is an Oracle 'holding' table for the most recent municipal estimates (v2015 at writing).  They are a control for evaluating the BGU table.

DLG_CTF_BGU_SDIST_TEMP - This is an Oracle 'holding' table for the most recent special district estimates (v2015 at writing).  They are a control for evaluating the BGU table.

DLG_CTF_BGU_TEMP - This is an Oracle 'holding' table for the most recent BGU estimates (v2015 at writing).

IT_County - This is the Access table that we use to populate the Oracle 'holding table' for counties.

IT_Muni - This is the Access table that we use to populate the Oracle 'holding table' for municipalities.

IT_district - This is the Access table that we use to populate the Oracle 'holding table' for special districts.

IT_BGU - This is the Access table that we use to populate the Oracle 'holding table' for BGUs.

### Macros

This is a section devoted to the various macros we use.  These don't really need to be run, I use them for final runs and when I'm only debugging a single section or input data, but for everything except BGU I test queries by hand.  I've removed many of the table delete commands, but if you're messing around, make sure to not use them.  At least make sure to copy the database to the desktop.

ReviewCopyExportMacro - this macro is used to export copies of the data to the estimates_review_app directory at J:\Estimates\Admin\AppDevelopment.  I use this app to evaluate each place's estimate once I'm finalizing.

RunAllUpdaters - This one, as it's name suggests, runs the entire estimates process in one fell swoop.  Don't run it if you don't mean it.  I use it for final estimates or when we get input changes.

StandardizationPull - This pulls nicely formatted and named data from the database Masters.  I don't currently use it, but keep it in case I ever want that data.

Step01_CountyEstimatesMaster_Updater - Runs all of the County Estimates Queries, must be run before other estimates queries.

Step02_AnnualConstructionData_Updater - Runs all queries to create the Annual Construction Data master we need to make City and County estimates.  This one is my most used Macro since we're usually not changing queries, we're updating input data.

Step03_CityandCountyEstimatesMaster_Updater - Runs all of the City and County Queries.

Step04-09_BGUEstimates20XX_RunAll - These Macros run each individual year for the BGU model.  We have to run it in year order because it requires the previous year to run to have inputs for the next year.  

Step09_BGUEstimatesOverlapped_RunAll - This runs all of the overlapped queries to create the important BGU Overlapped table.  This table has extra rows for overlapped areas to properly allocate the 

Step10_CTFEstimates_Final_RunAll -  This marco runs all the final estimates table for the special districts, this has to be run last.

### Estimates Input Data

This section contains all of the input data we need to run the model.  It's mostly linked Excel tables and Access tables.  If you want to see everything that is linked externally, use the 'Linked Table Manager'.  That will provide you file locations as well.  Use this to find the files if you need them, to make sure we're linking properly.



# Reports and Selectors



# Macros





## Archiving the Database

Toward the end of March through April it is time to think about archiving the old Database. This process has varied over time, but I have a procedure that seems to work well for me.  We used to archive the DB and ALL of the files associated, basically the whole directory.  That was a bit bloaty and sort of overkill. I've outlined my process below, it's pretty simple.

#### Archival Process
	1. Create a copy of the database on our local machine.  I rename it with the vitange name on the end, so for vintage 2014 it's "Estimates 2010-20v2014.accdb".
	2. Convert all of the linked tables (including those from Oracle) to local tables.  This makes the database self-contained and unable to be changed based on new data in the input files for the vintage. I highlight all of the linked tables in each section > right-click > then select "Convert to Local Table."
	3. Save the local database copy.
	4.  I like to keep the previous vintage in the main Estimates directory as well as archived.  So delete the previous vintage database already in the Estimates directory and then replace it with a copy of the one you just made.  BEFORE you do that, double check that the one you're going to delete is archived.
	5. Right-click on the local copy you've been using and compress the database into a zip file.  If you want to add other files to that, you can.  I did in 2013, but I'm not sure what purpose I thought that would serve. Rename the zipped file "VintageXXXX" based on what vintage you're archiving.
	6. Place the new zip file into the Archive sub-directory of Estimates. 
	7. You can delete the local copy.
	8. Send out an email to the staff to let them know what you've done and where to find the data. They'll appreciate it.
	9. Continue building new vintage in the DRAFT folder until you're ready then move it to the main directory.
